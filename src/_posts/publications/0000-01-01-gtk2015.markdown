---
title: "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy"
jemoji: ":star:"
layout: publication
category: publication
pubtype: conf
date: 2015-12-01
tag: 
    - publication
    - unsupervised learning
    - boltzmann machines
    - machine learning
authors: M. GabriÃ©, E. W. Tramel, & F. Krzakala
in: Neural and Information Processing Symposium (NIPS)
year: 2015
image: /assets/images/gtk2015.png
imageCaption: >-
    Estimates of the per-sample log-likelihood over the MNIST test set, normalized by the total number of units, as a function of the number of training epochs. The results for the different training algorithms are plotted in different colors with the same color code used for both panels. <b>Left panel:</b> Pseudo log-likelihood estimate. The difference between EMF algorithms and contrastive divergence algorithms is minimal. <b>Right panel:</b> EMF log-likelihood estimate at 2nd order. The improvement from MF to TAP is clear. Perhaps reasonably, TAP demonstrates an advantage over CD and PCD. Notice how the second-order EMF approximation of L provides less noisy estimates, at a lower computational cost.
link: http://papers.nips.cc/paper/5788-training-restricted-boltzmann-machine-via-the-thouless-anderson-palmer-free-energy
linkpdf: http://papers.nips.cc/paper/5788-training-restricted-boltzmann-machine-via-the-thouless-anderson-palmer-free-energy.pdf
linkcode: http://github.com/sphinxteam/Boltzmann.jl
linkposter: /assets/doc/gtk2015-poster.pdf
abstract: >-
    Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence,for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategy can be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.
bibtex: >-
    conference{GTZ2015,
        Address = {Montreal, Canada},
        Author = {Marylou Gabri{\'e} and Eric W. Tramel and Florent Krzakala},
        Booktitle = {Proc. Conf. on Neural Info. Processing Sys. (NIPS)},
        Month = {June},
        Title = {Training Restricted {B}oltzmann Machines via the {Thouless-Andreson-Palmer} Free Energy},
        Year = {2015}}
---